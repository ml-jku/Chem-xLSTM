{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q8s209Cw_Mw",
        "outputId": "ee659080-4220-432e-e23a-f0507f3116a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Chem-xLSTM'...\n",
            "remote: Enumerating objects: 107, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 107 (delta 17), reused 61 (delta 11), pack-reused 36 (from 3)\u001b[K\n",
            "Receiving objects: 100% (107/107), 83.74 MiB | 41.93 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n"
          ]
        }
      ],
      "source": [
        "# setup - don't forget to use GPU runtime setting\n",
        "!git clone https://github.com/ml-jku/Chem-xLSTM.git\n",
        "!apt-get install -y -qq openbabel\n",
        "!pip -q install rdkit prolif dask scikit-learn\n",
        "%cd Chem-xLSTM\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train an xLSTM model from scratch"
      ],
      "metadata": {
        "id": "BNjKzt6ao7_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# only for 3 epochs - but adjust to your liking\n",
        "!python chemxlstm/train.py --model_class xLSTM --model_dim 256 --state_dim 64 --n_layers 4 --n_heads 1 \\\n",
        "--n_max_epochs 3 --batch_size 512 --device cuda:0 --learning_rate 5e-3 --dropout 0.25 --vocab_size 37 \\\n",
        "--sequence_length 100 --patience 5 --delta 1e-5 --save_per_epoch 3 --no_denovodesign \\\n",
        "--logdir ./models/ --training_molecules_path ./data/chemblv31/train.zip --val_molecules_path ./data/chemblv31/valid.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6FXBmDlyW-p",
        "outputId": "75fc2fd4-8cf9-400d-ecd7-abf33c923dfb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory:  /content/Chem-xLSTM\n",
            "Total number of parameters:  1.644296 M\n",
            "Total number of (trainable) parameters:  1.644296 M\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory. Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Chem-xLSTM/wandb/offline-run-20251001_124207-610ifwsc\u001b[0m\n",
            "Time taken for one forward pass:  0.2344803810119629 s\n",
            "Input shape:  torch.Size([1, 100])\n",
            "Output shape:  torch.Size([1, 100, 37])\n",
            "Unsupported operator aten::embedding encountered 1 time(s)\n",
            "Unsupported operator aten::add encountered 29 time(s)\n",
            "Unsupported operator aten::silu encountered 8 time(s)\n",
            "Unsupported operator aten::log_sigmoid encountered 4 time(s)\n",
            "Unsupported operator aten::cumsum encountered 4 time(s)\n",
            "Unsupported operator aten::repeat encountered 4 time(s)\n",
            "Unsupported operator aten::sub encountered 8 time(s)\n",
            "Unsupported operator aten::where encountered 4 time(s)\n",
            "Unsupported operator aten::exp encountered 8 time(s)\n",
            "Unsupported operator aten::div encountered 8 time(s)\n",
            "Unsupported operator aten::mul encountered 20 time(s)\n",
            "Unsupported operator aten::sum encountered 4 time(s)\n",
            "Unsupported operator aten::abs encountered 4 time(s)\n",
            "Unsupported operator aten::neg encountered 4 time(s)\n",
            "Unsupported operator aten::maximum encountered 4 time(s)\n",
            "Total FLOPS (batch_size=2048, seq-len=):  0.205387776 GFLOPS\n",
            "100% 3711/3711 [49:57<00:00,  1.24it/s]\n",
            "Epoch:0\tLoss: 0.6618460857203243, Val Loss: 0.605961266829043\n",
            " 60% 2238/3711 [30:11<19:52,  1.24it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Chem-xLSTM/chemxlstm/train.py\", line 241, in <module>\n",
            "    model = main(args)\n",
            "            ^^^^^^^^^^\n",
            "  File \"/content/Chem-xLSTM/chemxlstm/train.py\", line 166, in main\n",
            "    history = model.train(\n",
            "              ^^^^^^^^^^^^\n",
            "  File \"/content/Chem-xLSTM/chemxlstm/model.py\", line 829, in train\n",
            "    epoch_train_loss += batch_train_loss.item()\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[1;34mwandb\u001b[0m: \u001b[1mwandb sync /content/Chem-xLSTM/wandb/offline-run-20251001_124207-610ifwsc\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/offline-run-20251001_124207-610ifwsc/logs\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few-Shot Conditinoal Generation with a pretrained model\n"
      ],
      "metadata": {
        "id": "F4GmlYPZE81Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the pretrained models extract them and put them into models folder\n",
        "!wget https://cloud.ml.jku.at/s/qpAS9iftYCN95by/download -O 'model.zip'\n",
        "!unzip model.zip -d models\n",
        "!unzip ./models/chem-xlstm-share/models.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_MHDEnEQcAD",
        "outputId": "28ca8cf2-d1f3-4615-a80d-43243d8fd5c9"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-01 15:50:19--  https://cloud.ml.jku.at/s/qpAS9iftYCN95by/download\n",
            "Resolving cloud.ml.jku.at (cloud.ml.jku.at)... 140.78.90.41\n",
            "Connecting to cloud.ml.jku.at (cloud.ml.jku.at)|140.78.90.41|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘model.zip’\n",
            "\n",
            "model.zip               [        <=>         ] 105.54M  20.3MB/s    in 6.3s    \n",
            "\n",
            "2025-10-01 15:50:27 (16.7 MB/s) - ‘model.zip’ saved [110669693]\n",
            "\n",
            "Archive:  model.zip\n",
            "   creating: models/chem-xlstm-share/\n",
            " extracting: models/chem-xlstm-share/models.zip  \n",
            "Archive:  ./models/chem-xlstm-share/models.zip\n",
            "   creating: models/chemblv31_pretrained/\n",
            "   creating: models/chemblv31_pretrained/xLSTM-14.8M-ed512_hid64_l9_he8_267/\n",
            "  inflating: models/chemblv31_pretrained/xLSTM-14.8M-ed512_hid64_l9_he8_267/init_arguments.json  \n",
            "  inflating: models/chemblv31_pretrained/xLSTM-14.8M-ed512_hid64_l9_he8_267/model.pt  \n",
            "   creating: models/icst_pretrained/\n",
            "   creating: models/icst_pretrained/xLSTM-15.0M-ed512_hid64_l9_he8_500/\n",
            "  inflating: models/icst_pretrained/xLSTM-15.0M-ed512_hid64_l9_he8_500/init_arguments.json  \n",
            "  inflating: models/icst_pretrained/xLSTM-15.0M-ed512_hid64_l9_he8_500/model.pt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from chemxlstm import xLSTMforNTP, LSTMforNTP, MambaforNTP, GPTforNTP, xLSTMforNTP\n",
        "model_path = '/content/Chem-xLSTM/models/icst_pretrained/xLSTM-15.0M-ed512_hid64_l9_he8_500'\n",
        "device = 'cuda:0'\n",
        "model = xLSTMforNTP.from_file(model_path, n_heads=8, device=device) #n_ssm has to be set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7_wlht3PJre",
        "outputId": "e46f5a48-1402-4ade-8fb0-d3d600cd4345"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/Chem-xLSTM/models/icst_pretrained/xLSTM-15.0M-ed512_hid64_l9_he8_500, using model class: xLSTMModel\n",
            "Properties: {'mode': 'smiles', 'model_dim': 512, 'state_dim': 64, 'n_layers': 9, 'n_ssm': 1, 'dropout': 0.25, 'vocab_size': 200, 'sequence_length': 128, 'n_max_epochs': 100, 'learning_rate': 0.001, 'batch_size': 1, 'device': 'cuda:0', 'n_augmentations': 0, 'gpt_upj_factor': 4.0, 'kwargs': {}, 'warned_about_token': [], 'token2label': {'[PAD]': 0, '[BEG]': 1, 'C': 2, 'c': 3, '1': 4, '(': 5, '=': 6, 'O': 7, ')': 8, 'n': 9, '-': 10, '2': 11, '.': 12, 'Cl': 13, 'N': 14, '3': 15, 'S': 16, '/': 17, '[': 18, '+': 19, ']': 20, 'P': 21, '@': 22, 'H': 23, 'o': 24, 'F': 25, 'Br': 26, '#': 27, '4': 28, '5': 29, 'Na': 30, '6': 31, '7': 32, 's': 33, 'Si': 34, 'I': 35, 'K': 36, 'Au': 37, 'Pt': 38, 'Se': 39, 'Fe': 40, '8': 41, 'se': 42, 'Li': 43, '[END]': 44, 'B': 45, '9': 46, 'Cu': 47, 'Te': 48, 'As': 49, 'Ni': 50, 'Sb': 51, 'Ba': 52, 'Mn': 53, 'Ag': 54, 'Pb': 55, 'Eu': 56, 'Ca': 57, 'Zn': 58, 'Ti': 59, 'Al': 60, '*': 61, 'Bi': 62, '0': 63, 'Mg': 64, '%10': 65, 'U': 66, 'Sr': 67, 'Gd': 68, 'Cf': 69, 'Sm': 70, 'Ra': 71, 'La': 72, '%11': 73, 'V': 74, 'p': 75, 'Cs': 76, 'Dy': 77, 'b': 78, 'Be': 79, 'Nd': 80, 'Lu': 81, 'Th': 82, 'Ac': 83, 'He': 84, 'Xe': 85, 'Rb': 86, 'Kr': 87, 'Ne': 88, 'Er': 89, 'Lr': 90, 'te': 91, '%12': 92, '%13': 93, '%14': 94, '%15': 95, '%16': 96, '%17': 97, '%18': 98, '%19': 99, '%20': 100, '%21': 101, '%22': 102, '%23': 103, '%24': 104}, 'label2token': {'0': '[PAD]', '1': '[BEG]', '2': 'C', '3': 'c', '4': '1', '5': '(', '6': '=', '7': 'O', '8': ')', '9': 'n', '10': '-', '11': '2', '12': '.', '13': 'Cl', '14': 'N', '15': '3', '16': 'S', '17': '/', '18': '[', '19': '+', '20': ']', '21': 'P', '22': '@', '23': 'H', '24': 'o', '25': 'F', '26': 'Br', '27': '#', '28': '4', '29': '5', '30': 'Na', '31': '6', '32': '7', '33': 's', '34': 'Si', '35': 'I', '36': 'K', '37': 'Au', '38': 'Pt', '39': 'Se', '40': 'Fe', '41': '8', '42': 'se', '43': 'Li', '44': '[END]', '45': 'B', '46': '9', '47': 'Cu', '48': 'Te', '49': 'As', '50': 'Ni', '51': 'Sb', '52': 'Ba', '53': 'Mn', '54': 'Ag', '55': 'Pb', '56': 'Eu', '57': 'Ca', '58': 'Zn', '59': 'Ti', '60': 'Al', '61': '*', '62': 'Bi', '63': '0', '64': 'Mg', '65': '%10', '66': 'U', '67': 'Sr', '68': 'Gd', '69': 'Cf', '70': 'Sm', '71': 'Ra', '72': 'La', '73': '%11', '74': 'V', '75': 'p', '76': 'Cs', '77': 'Dy', '78': 'b', '79': 'Be', '80': 'Nd', '81': 'Lu', '82': 'Th', '83': 'Ac', '84': 'He', '85': 'Xe', '86': 'Rb', '87': 'Kr', '88': 'Ne', '89': 'Er', '90': 'Lr', '91': 'te', '92': '%12', '93': '%13', '94': '%14', '95': '%15', '96': '%16', '97': '%17', '98': '%18', '99': '%19', '100': '%20', '101': '%21', '102': '%22', '103': '%23', '104': '%24'}, 'model_class': \"<class 's4dd.s4_for_denovo_design.xLSTMModel'>\", 'n_heads': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in the context put in the few samples form a distribution from which you want the designs to come from\n",
        "seq, ll = model.design_molecules(n_designs=3, batch_size=1, temperature=1.4, context='CCCCCCCCCCCCCCC.CCCCCCCCCCCC.', debug=False, sequence_length=500)\n",
        "seq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oha72Nl3Pc14",
        "outputId": "56fe2778-3457-4793-bd6e-bb05076189bc"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:22<00:00,  7.66s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['CCCCCCCCCCCCCCC.CCCCCCCCCCCC.Clc1coo1)CNC2=CCN(c3ccco3)CC1(=O)%21NS(=O)(CO)(C(C)(OC(C)O)c(=O)n(CC)Cl)c1=O',\n",
              " 'CCCCCCCCCCCCCCC.CCCCCCCCCCCC.C=[Pt+].C/C(C(Cl)F)OCCl)O[].CC#N-].Cl.[S-].C1=CC=C(C=C1)CC(=O)NIC(=O)NC2=C(C=CC(=C2)Br)ClNO)C=NC1=CCCN1C=NC2=CC=CN2)C.C(=O)(=O)C(=O)NC.Cl',\n",
              " 'CCCCCCCCCCCCCCC.CCCCCCCCCCCC.CC1=CC(=C(C=C1)SC2=NC1=C(C=n1)C3CCcc(C3(C)CC45N(COC(C4C3=CC(C4CCCCC4)nc(N[[UNK][UNK]']"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    }
  ]
}
